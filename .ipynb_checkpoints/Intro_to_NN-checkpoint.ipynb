{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical workflow\n",
    "Define Dataset, Dataloader -> Define Model -> Train -> Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=tf.ToTensor())\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=tf.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset stores the samples and their corresponding labels while DataLoader wraps an iterable around the dataset\n",
    "\n",
    "- Batch size = number of datasamples propogated before parameters are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Using the dataloader\n",
    "\n",
    "loader = enumerate(train_dl)\n",
    "batch, (x,y) = next(loader)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[0][0], cmap='gray')\n",
    "plt.show()\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (MLP)\n",
    "- Model does not return probabilities in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) tensor([[0.4927, 0.4840, 0.4635, 0.4743, 0.4793, 0.5072, 0.4866, 0.5058, 0.5223,\n",
      "         0.4762]])\n",
      "tensor([8])\n"
     ]
    }
   ],
   "source": [
    "# Looking at models outputs using a random input tensor\n",
    "def test():\n",
    "    model = MLP()\n",
    "    x = torch.randn(1, 1, 28, 28)\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    print(y.shape, y)\n",
    "    \n",
    "    # evaluating probabilites\n",
    "    print(y.argmax(1))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "#### Hyperparameters\n",
    "- adjustable params that you control optimisation process\n",
    "\n",
    "- Epochs = number of times to iterate over dataset\n",
    "- Learning Rate= how much to update the model paramters at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = MLP()\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device=DEVICE)\n",
    "        y = y.to(device=DEVICE)\n",
    "        #compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backpropogation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%200 ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device=DEVICE)\n",
    "            y = y.to(device=DEVICE)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "loss: 2.302954  [    0/60000]\n",
      "loss: 1.557730  [12800/60000]\n",
      "loss: 1.527623  [25600/60000]\n",
      "loss: 1.510439  [38400/60000]\n",
      "loss: 1.497124  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 1.501901 \n",
      "\n",
      "Epoch 2\n",
      " ++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "loss: 1.495260  [    0/60000]\n",
      "loss: 1.466637  [12800/60000]\n",
      "loss: 1.493652  [25600/60000]\n",
      "loss: 1.496328  [38400/60000]\n",
      "loss: 1.519226  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 1.488569 \n",
      "\n",
      "Epoch 3\n",
      " ++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "loss: 1.486113  [    0/60000]\n",
      "loss: 1.469841  [12800/60000]\n",
      "loss: 1.494673  [25600/60000]\n",
      "loss: 1.480494  [38400/60000]\n",
      "loss: 1.484564  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 1.484647 \n",
      "\n",
      "Epoch 4\n",
      " ++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "loss: 1.476008  [    0/60000]\n",
      "loss: 1.484533  [12800/60000]\n",
      "loss: 1.493695  [25600/60000]\n",
      "loss: 1.474375  [38400/60000]\n",
      "loss: 1.477496  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 1.482900 \n",
      "\n",
      "Epoch 5\n",
      " ++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "loss: 1.470822  [    0/60000]\n",
      "loss: 1.467798  [12800/60000]\n",
      "loss: 1.488848  [25600/60000]\n",
      "loss: 1.469249  [38400/60000]\n",
      "loss: 1.484046  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 1.484150 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device=DEVICE)\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n ++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    train_loop(train_dl, model, loss_fn, optimizer)\n",
    "    test_loop(test_dl, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting classes\n",
    "- Using our trained model to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#image = Image.open('./topredict.jpeg').convert(\"L\")\n",
    "images = {x.split(\".\")[0]:cv2.bitwise_not(cv2.cvtColor(np.array(Image.open( \"./to_predict/\" + x)), cv2.COLOR_BGR2GRAY)) for x in os.listdir(\"./to_predict\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "transform = tf.Compose([\n",
    "    tf.ToTensor(),\n",
    "    tf.Resize((28,28))\n",
    "])\n",
    "\n",
    "transformed_images = {}\n",
    "\n",
    "for image in images.keys():\n",
    "    images[image] = cv2.resize(images[image], (28, 28))\n",
    "    transformed_images[image] = transform(images[image])\n",
    "\n",
    "#image_tensor = transform(image)\n",
    "#image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real \t:\t Pred\n",
      "\n",
      "THREE \t:\t 3\n",
      "SIX \t:\t 5\n",
      "TWO \t:\t 3\n",
      "FOUR \t:\t 8\n",
      "ZERO \t:\t 0\n",
      "EIGHT \t:\t 3\n",
      "FIVE \t:\t 3\n",
      "ONE \t:\t 8\n",
      "SEVEN \t:\t 3\n",
      "NINE \t:\t 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Real\", \"\\t:\\t\", \"Pred\" + \"\\n\")\n",
    "for key, image_tensor in transformed_images.items():\n",
    "    image_tensor = image_tensor.resize(1, 1, 28, 28)\n",
    "    image_tensor = image_tensor.to(device=DEVICE)\n",
    "    pred = model(image_tensor)\n",
    "    print(key.upper(), \"\\t:\\t\", int(torch.argmax(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
